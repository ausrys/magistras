{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import vstack\n",
    "import spacy\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "nlp = spacy.load(\"lt_core_news_lg\")\n",
    "all_values = pd.read_csv('pradinisi.csv', encoding='utf-8', usecols=['text', 'class']).values\n",
    "stopwordsall = pd.read_csv('stopwords.csv')\n",
    "stopwords = stopwordsall['stop_words'].tolist()\n",
    "stop_words_set = set(stopwords)\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "import re\n",
    "import pickle\n",
    "import snowballstemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "stemmer = snowballstemmer.stemmer('lithuanian')\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_values = pd.read_csv('pradinisi.csv', usecols=['text', 'class'])\n",
    "filtered_valuesY = all_values.loc[all_values['class'] == 1, 'text'].values\n",
    "filtered_valuesX = all_values.loc[all_values['class'] == 0, 'text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number of words: 14\n",
      "Maximum number of words: 320\n",
      "Average number of words: 83.38461538461539\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum number of words: 1\n",
      "Maximum number of words: 30\n",
      "Average number of words: 8.52662721893491\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import snowballstemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "stemmer = snowballstemmer.stemmer('lithuanian')\n",
    "nlp = spacy.load(\"lt_core_news_lg\")\n",
    "all_values = pd.read_csv('pradinisi.csv', encoding='utf-8', usecols=['text', 'class']).values\n",
    "stopwordsall = pd.read_csv('stopwords.csv')\n",
    "stopwords = stopwordsall['stop_words'].tolist()\n",
    "stop_words_set = set(stopwords)\n",
    "documents = []\n",
    "documents_be = []\n",
    "maxtokenX = 1\n",
    "# Initialize variables\n",
    "min_words = float('inf')\n",
    "max_words = float('-inf')\n",
    "total_words = 0\n",
    "\n",
    "for sen in (filtered_valuesY):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(sen))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    document = re.sub(\"\\d+\", \"\", document)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    document = document.split()\n",
    "    documentbe = str(sen).split()\n",
    "    document = [word for word in document if not word in stop_words_set]\n",
    "    # Stemming\n",
    "    document = ' '.join(document)\n",
    "    document = nlp(document)\n",
    "    document = [token.orth_ for token in document]\n",
    "    document = stemmer.stemWords(document)\n",
    "    num_words = len(document)\n",
    "    min_words = min(min_words, num_words)\n",
    "    max_words = max(max_words, num_words)\n",
    "    total_words += num_words\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)\n",
    "    # Calculate average\n",
    "average_words = total_words / len(filtered_valuesX)\n",
    "# Print the results\n",
    "print(\"Minimum number of words:\", min_words)\n",
    "print(\"Maximum number of words:\", max_words)\n",
    "print(\"Average number of words:\", average_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.043392504930967 12.284023668639053\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_corection(sentence):\n",
    "    \n",
    "    document = re.sub(r'\\W', ' ', str(sentence))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    document = re.sub(\"\\d+\", \"\", document)\n",
    "\n",
    "    # Stop words filter\n",
    "    document = document.split()\n",
    "    document = [word for word in document if not word in stop_words_set]\n",
    "    # Stemming\n",
    "    document = ' '.join(document)\n",
    "    document = nlp(document)\n",
    "    document = [token.orth_ for token in document]\n",
    "    document = stemmer.stemWords(document)\n",
    "    document = ' '.join(document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(sentences):\n",
    "    y = [] # sentences classes\n",
    "    for sentence in sentences:\n",
    "        y.append(sentence[1]) # sentences classes\n",
    "    return y\n",
    "y = get_splits(all_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [507, 1014]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ausry\\Desktop\\Magis\\fullproject.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ausry/Desktop/Magis/fullproject.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m vectorizer \u001b[39m=\u001b[39m CountVectorizer()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ausry/Desktop/Magis/fullproject.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m X \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mfit_transform(documents)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ausry/Desktop/Magis/fullproject.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, stratify\u001b[39m=\u001b[39;49my,)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ausry/Desktop/Magis/fullproject.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Inverse\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ausry/Desktop/Magis/fullproject.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m vectorizerI \u001b[39m=\u001b[39m TfidfVectorizer(max_features\u001b[39m=\u001b[39m\u001b[39m4000\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2559\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2556\u001b[0m \u001b[39mif\u001b[39;00m n_arrays \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2557\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAt least one array required as input\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2559\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39;49marrays)\n\u001b[0;32m   2561\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m   2562\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m\n\u001b[0;32m   2564\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:443\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \n\u001b[0;32m    426\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    442\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[1;32m--> 443\u001b[0m check_consistent_length(\u001b[39m*\u001b[39;49mresult)\n\u001b[0;32m    444\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [507, 1014]"
     ]
    }
   ],
   "source": [
    "# Vectorize the text data using CounterVectorizer (Bag of words)\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0, shuffle=True, stratify=y,)\n",
    "# Inverse\n",
    "vectorizerI = TfidfVectorizer(max_features=4000)\n",
    "Inverse = vectorizerI.fit_transform(documents)\n",
    "X_train_I, X_test_I, y_train_I, y_test_I = train_test_split(Inverse, y, test_size=0.25, random_state=0, shuffle=True, stratify=y,)\n",
    "# Apply Latent Semantic Analysis (LSA) to reduce dimensionality\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# Bag of words\n",
    "lsa = TruncatedSVD(n_components=450)\n",
    "train_features = lsa.fit_transform(X_train)\n",
    "test_features = lsa.transform(X_test)\n",
    "# Inverse\n",
    "lsa = TruncatedSVD(n_components=450)\n",
    "train_features_I = lsa.fit_transform(X_train_I)\n",
    "test_features_I = lsa.transform(X_test_I)\n",
    "pickle.dump(vectorizer, open('vectorizer.pkl', 'wb'))\n",
    "pickle.dump(vectorizerI, open('vetorizerI.pkl', 'wb'))\n",
    "pickle.dump(lsa, open('lsa.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy='auto')\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "vectorizerI = TfidfVectorizer(max_features=4000)\n",
    "Inverse = vectorizerI.fit_transform(documents)\n",
    "lsa = TruncatedSVD(n_components=600)\n",
    "lsa2 = TruncatedSVD(n_components=600)\n",
    "# TF\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "new_y = y +y_smote\n",
    "S_TF = vstack([X, X_smote])\n",
    "X_train, X_test, y_train, y_test = train_test_split(S_TF, new_y, test_size=0.25, random_state=0, shuffle=True, stratify=new_y)\n",
    "# TF-IDF\n",
    "X_smote_I, y_smote_I = smote.fit_resample(Inverse, y)\n",
    "S_TF_I = vstack([Inverse, X_smote_I])\n",
    "smote_inv_y = y + y_smote_I\n",
    "X_train_I, X_test_I, y_train_I, y_test_I = train_test_split(S_TF_I, smote_inv_y, test_size=0.25, random_state=0, shuffle=True, stratify=smote_inv_y,)\n",
    "train_features = lsa.fit_transform(X_train)\n",
    "test_features = lsa.transform(X_test)\n",
    "train_features_I = lsa2.fit_transform(X_train_I)\n",
    "test_features_I = lsa2.transform(X_test_I)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smote before vectors\n",
    "smote = SMOTE(sampling_strategy='all', k_neighbors=450)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "smote2 = SMOTE(sampling_strategy='all', k_neighbors=450)\n",
    "X_smote2, y_smote2 = smote.fit_resample(Inverse, y)\n",
    "new_X = vectorizer.inverse_transform(X_smote)\n",
    "new_X2 = vectorizer.inverse_transform(X_smote2)\n",
    "print(f\"Originalus sakinys: {documents[100]}\")\n",
    "print(f\"SMOTE ir TF: {' '.join(new_X[100])}\")\n",
    "print(f\"Originalus sakinys: {documents[100]}\")\n",
    "print(f\"SMOTE ir TF-IDF: {' '.join(new_X2[100])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifiers on the reduced feature matrix\n",
    "# SVM with LSA\n",
    "svmLSA = SVC(C=0.9, kernel='linear', degree=3, gamma='auto')\n",
    "svmLSA.fit(train_features, y_train)\n",
    "\n",
    "# SVM with LSA and Inv\n",
    "svmLSA_Inverse = SVC(C=0.9, kernel='linear', degree=3, gamma='auto')\n",
    "svmLSA_Inverse.fit(train_features_I, y_train_I)\n",
    "\n",
    "# SVM withoutLSA\n",
    "svm = SVC(C=0.9, kernel='linear', degree=3, gamma='auto')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# SVM withoutLSA with INV\n",
    "svm_Inverse = SVC(C=0.9, kernel='linear', degree=3, gamma='auto')\n",
    "svm_Inverse.fit(X_train_I, y_train_I)\n",
    "\n",
    "# Predict labels for test data with LSA\n",
    "predicted_labels_SVM_LSA = svmLSA.predict(test_features)\n",
    "\n",
    "# Predict labels for test data\n",
    "predicted_labels_SVM = svm.predict(X_test)\n",
    "\n",
    "# Predict labels for test data with LSA and Inverse\n",
    "predicted_labels_SVM_LSA_I = svmLSA_Inverse.predict(test_features_I)\n",
    "\n",
    "# Predict labels for test data\n",
    "predicted_labels_SVM_I = svm_Inverse.predict(X_test_I)\n",
    "print(predicted_labels_SVM_I)\n",
    "print(y_test_I)\n",
    "# print(vectorizerI.inverse_transform(X_train_I[12]))\n",
    "# Evaluate  SVM classifier with LSA\n",
    "SVM_accuracy_LSA = accuracy_score(y_test, predicted_labels_SVM_LSA)\n",
    "SVM_report_LSA = classification_report(y_test, predicted_labels_SVM_LSA)\n",
    "SVM_matrix_LSA = confusion_matrix(y_test, predicted_labels_SVM_LSA)\n",
    "# Evaluate  SVM classifier\n",
    "SVM_accuracy = accuracy_score(y_test, predicted_labels_SVM)\n",
    "SVM_report = classification_report(y_test, predicted_labels_SVM)\n",
    "SVM_matrix = confusion_matrix(y_test, predicted_labels_SVM)\n",
    "# Evaluate  SVM classifier with Inverse and LSA\n",
    "SVM_accuracy_LSA_I = accuracy_score(y_test_I, predicted_labels_SVM_LSA_I)\n",
    "SVM_report_LSA_I = classification_report(y_test_I, predicted_labels_SVM_LSA_I)\n",
    "SVM_matrix_LSA_I = confusion_matrix(y_test_I, predicted_labels_SVM_LSA_I)\n",
    "# Evaluate  SVM classifier with Inverse\n",
    "SVM_accuracy_I = accuracy_score(y_test_I, predicted_labels_SVM_I)\n",
    "SVM_report_I = classification_report(y_test_I, predicted_labels_SVM_I)\n",
    "SVM_matrix_I = confusion_matrix(y_test_I, predicted_labels_SVM_I)\n",
    "# Plot conf matrixes\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12,4))\n",
    "ax = ax.flatten()\n",
    "svmMat = sns.heatmap(SVM_matrix, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[0])\n",
    "svmMatLSA = sns.heatmap(SVM_matrix_LSA, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[1])\n",
    "svmMat_I = sns.heatmap(SVM_matrix_I, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[2])\n",
    "svmMatLSA_I = sns.heatmap(SVM_matrix_LSA_I, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[3])\n",
    "ax[0].set_title('SVM TF')\n",
    "ax[0].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[0].set_ylabel('Tikra klasė', fontsize=10)\n",
    "ax[1].set_title('SVM TF ir LSA')\n",
    "ax[1].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[1].set_ylabel('Tikra klasė', fontsize=10)\n",
    "ax[2].set_title('SVM TF-IDF')\n",
    "ax[2].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[2].set_ylabel('Tikra klasė', fontsize=10)\n",
    "ax[3].set_title('SVM TF-IDF ir LSA')\n",
    "ax[3].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[3].set_ylabel('Tikra klasė', fontsize=10)\n",
    "# Adjust spacing between subplots and between subplots and figure edges\n",
    "# Add a title to the figure\n",
    "plt.subplots_adjust(wspace=0.25, hspace=0.8, top=0.92, bottom=0.08, left=0.08, right=0.92)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Evaluate  SVM classifier\n",
    "print(f\"SVM \\n Accuracy: {SVM_accuracy:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",SVM_report)\n",
    "print(f\"SVM LSA \\n Accuracy: {SVM_accuracy_LSA:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",SVM_report_LSA)\n",
    "print(f\"SVM Inverse \\n Accuracy: {SVM_accuracy_I:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",SVM_report_I)\n",
    "print(f\"SVM Inverse LSA \\n Accuracy: {SVM_accuracy_LSA_I:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",SVM_report_LSA_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM dimensions\n",
    "# Bag of words\n",
    "# Define the range of number of components\n",
    "n_components_range = range(50, 1001, 50)\n",
    "accuracies = []\n",
    "reports = []\n",
    "matrices = []\n",
    "accuraciesI = []\n",
    "reportsI = []\n",
    "matricesI = []\n",
    "# Initialize lists to store evaluation results\n",
    "for n_components in n_components_range:\n",
    "    lsa = TruncatedSVD(n_components=n_components)\n",
    "    train_features = lsa.fit_transform(X_train)\n",
    "    test_features = lsa.transform(X_test)\n",
    "    lsa = TruncatedSVD(n_components=n_components)\n",
    "    train_features_I = lsa.fit_transform(X_train_I)\n",
    "    test_features_I = lsa.transform(X_test_I)\n",
    "    svmLSA = SVC(C=0.9, kernel='linear', degree=3, gamma='auto')\n",
    "    svmLSA.fit(train_features, y_train)\n",
    "    svmLSA_Inverse = SVC(C=0.9, kernel='linear', degree=3, gamma='auto')\n",
    "    svmLSA_Inverse.fit(train_features_I, y_train_I)\n",
    "    predicted_labels_SVM_LSA = svmLSA.predict(test_features)\n",
    "    predicted_labels_SVM_LSA_I = svmLSA_Inverse.predict(test_features_I)\n",
    "    SVM_accuracy_LSA = accuracy_score(y_test, predicted_labels_SVM_LSA)\n",
    "    SVM_report_LSA = classification_report(y_test, predicted_labels_SVM_LSA)\n",
    "    SVM_matrix_LSA = confusion_matrix(y_test, predicted_labels_SVM_LSA)\n",
    "    SVM_accuracy_LSA_I = accuracy_score(y_test_I, predicted_labels_SVM_LSA_I)\n",
    "    SVM_report_LSA_I = classification_report(y_test_I, predicted_labels_SVM_LSA_I)\n",
    "    SVM_matrix_LSA_I = confusion_matrix(y_test_I, predicted_labels_SVM_LSA_I)\n",
    "    accuracies.append(SVM_accuracy_LSA)\n",
    "    reports.append(SVM_report_LSA)\n",
    "    matrices.append(SVM_matrix_LSA)\n",
    "    accuraciesI.append(SVM_accuracy_LSA_I)\n",
    "    reportsI.append(SVM_report_LSA_I)\n",
    "    matricesI.append(SVM_matrix_LSA_I)\n",
    "for accuracy in accuracies:\n",
    "    print(accuracy)\n",
    "for accuracy in accuraciesI:\n",
    "    print(accuracy)\n",
    "# Print all the evaluation results\n",
    "# for n_components, SVM_accuracy_LSA, SVM_report_LSA, SVM_matrix_LSA,SVM_accuracy_LSA_I,SVM_report_LSA_I,SVM_matrix_LSA_I in zip(n_components_range, accuracies, reports, matrices,accuraciesI, reportsI, matricesI):\n",
    "#     print(f\"n_components = {n_components}\")\n",
    "#     print(f\"Accuracy: {SVM_accuracy_LSA}\")\n",
    "#     print(f\"Classification Report:\\n{SVM_report_LSA}\")\n",
    "#     print(f\"Confusion Matrix:\\n{SVM_matrix_LSA}\\n\")\n",
    "#     print(f\"Accuracy I : {SVM_accuracy_LSA_I}\")\n",
    "#     print(f\"Classification Report I :\\n{SVM_report_LSA_I}\")\n",
    "#     print(f\"Confusion Matrix I :\\n{SVM_matrix_LSA_I}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "random = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "random.fit(X_train, y_train)\n",
    "\n",
    "# Random Forest with LSA\n",
    "randomLSA = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "randomLSA.fit(train_features, y_train)\n",
    "\n",
    "# Random Forest Inverse\n",
    "random_I = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "random_I.fit(X_train_I, y_train_I)\n",
    "\n",
    "# Random Forest with LSA and INV\n",
    "randomLSA_I = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "randomLSA_I.fit(train_features_I, y_train_I)\n",
    "\n",
    "# Predict labels for test data with LSA\n",
    "predicted_labels_Random_LSA = randomLSA.predict(test_features)\n",
    "\n",
    "# Predict labels for test data\n",
    "predicted_labels_Random = random.predict(X_test)\n",
    "\n",
    "# Predict labels for test data with LSA and Inverse\n",
    "predicted_labels_Random_LSA_I = randomLSA_I.predict(test_features_I)\n",
    "\n",
    "# Predict labels for test data\n",
    "predicted_labels_Random_I = random_I.predict(X_test_I)\n",
    "\n",
    "# Evaluate  Random classifier with LSA\n",
    "Random_accuracy_LSA = accuracy_score(y_test, predicted_labels_Random_LSA)\n",
    "Random_report_LSA = classification_report(y_test, predicted_labels_Random_LSA)\n",
    "Random_matrix_LSA = confusion_matrix(y_test, predicted_labels_Random_LSA)\n",
    "# Evaluate  Random classifier\n",
    "Random_accuracy = accuracy_score(y_test, predicted_labels_Random)\n",
    "Random_report = classification_report(y_test, predicted_labels_Random)\n",
    "Random_matrix = confusion_matrix(y_test, predicted_labels_Random)\n",
    "# Evaluate  Random classifier with Inverse and LSA\n",
    "Random_accuracy_LSA_I = accuracy_score(y_test_I, predicted_labels_Random_LSA_I)\n",
    "Random_report_LSA_I = classification_report(y_test_I, predicted_labels_Random_LSA_I)\n",
    "Random_matrix_LSA_I = confusion_matrix(y_test_I, predicted_labels_Random_LSA_I)\n",
    "# Evaluate  Randonm classifier with Inverse\n",
    "Random_accuracy_I = accuracy_score(y_test_I, predicted_labels_Random_I)\n",
    "Random_report_I = classification_report(y_test_I, predicted_labels_Random_I)\n",
    "Random_matrix_I = confusion_matrix(y_test_I, predicted_labels_Random_I)\n",
    "# Plot conf matrixes\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12,4))\n",
    "ax = ax.flatten()\n",
    "RandomMat = sns.heatmap(Random_matrix, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[0])\n",
    "RandomMatLSA = sns.heatmap(Random_matrix_LSA, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[1])\n",
    "RandomMat_I = sns.heatmap(Random_matrix_I, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[2])\n",
    "RandomMatLSA_I = sns.heatmap(Random_matrix_LSA_I, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[3])\n",
    "ax[0].set_title('Random Forest TF')\n",
    "ax[0].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[0].set_ylabel('Tikra klasė', fontsize=10)\n",
    "ax[1].set_title('Random Forest TF ir LSA')\n",
    "ax[1].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[1].set_ylabel('Tikra klasė', fontsize=10)\n",
    "ax[2].set_title('Random Forest TF-IDF')\n",
    "ax[2].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[2].set_ylabel('Tikra klasė', fontsize=10)\n",
    "ax[3].set_title('Random Forest TF-IDF ir LSA')\n",
    "ax[3].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[3].set_ylabel('Tikra klasė', fontsize=10)\n",
    "# Adjust spacing between subplots and between subplots and figure edges\n",
    "# Add a title to the figure\n",
    "plt.subplots_adjust(wspace=0.25, hspace=0.7, top=0.92, bottom=0.08, left=0.08, right=0.92)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Print Statistics\n",
    "print(f\"Random \\n Accuracy: {Random_accuracy:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",Random_report)\n",
    "print(f\"Random LSA \\n Accuracy: {Random_accuracy_LSA:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",Random_report_LSA)\n",
    "print(f\"Random Inverse \\n Accuracy: {Random_accuracy_I:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",Random_report_I)\n",
    "print(f\"Random Inverse LSA \\n Accuracy: {Random_accuracy_LSA_I:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",Random_report_LSA_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components_range = range(50, 1001, 50)\n",
    "accuracies = []\n",
    "reports = []\n",
    "matrices = []\n",
    "accuraciesI = []\n",
    "reportsI = []\n",
    "matricesI = []\n",
    "# Initialize lists to store evaluation results\n",
    "for n_components in n_components_range:\n",
    "    lsa = TruncatedSVD(n_components=n_components)\n",
    "    train_features = lsa.fit_transform(X_train)\n",
    "    test_features = lsa.transform(X_test)\n",
    "    lsa = TruncatedSVD(n_components=n_components)\n",
    "    train_features_I = lsa.fit_transform(X_train_I)\n",
    "    test_features_I = lsa.transform(X_test_I)\n",
    "    RandomLSA = RandomForestClassifier(n_estimators=150, max_depth=10)\n",
    "    RandomLSA.fit(train_features, y_train)\n",
    "    RandomLSA_Inverse = RandomForestClassifier(n_estimators=150, max_depth=10)\n",
    "    RandomLSA_Inverse.fit(train_features_I, y_train_I)\n",
    "    predicted_labels_Random_LSA = RandomLSA.predict(test_features)\n",
    "    predicted_labels_Random_LSA_I = RandomLSA_Inverse.predict(test_features_I)\n",
    "    Random_accuracy_LSA = accuracy_score(y_test, predicted_labels_Random_LSA)\n",
    "    Random_report_LSA = classification_report(y_test, predicted_labels_Random_LSA)\n",
    "    Random_matrix_LSA = confusion_matrix(y_test, predicted_labels_Random_LSA)\n",
    "    Random_accuracy_LSA_I = accuracy_score(y_test_I, predicted_labels_Random_LSA_I)\n",
    "    Random_report_LSA_I = classification_report(y_test_I, predicted_labels_Random_LSA_I)\n",
    "    Random_matrix_LSA_I = confusion_matrix(y_test_I, predicted_labels_Random_LSA_I)\n",
    "    accuracies.append(Random_accuracy_LSA)\n",
    "    reports.append(Random_report_LSA)\n",
    "    matrices.append(Random_matrix_LSA)\n",
    "    accuraciesI.append(Random_accuracy_LSA_I)\n",
    "    reportsI.append(Random_report_LSA_I)\n",
    "    matricesI.append(Random_matrix_LSA_I)\n",
    "for accuracy in accuracies:\n",
    "    print(accuracy)\n",
    "for accuracy in accuraciesI:\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value = np.min(train_features)\n",
    "train_features = train_features + abs(min_value)\n",
    "min_valueT = np.min(test_features)\n",
    "test_features = test_features + abs(min_valueT)\n",
    "min_valueI = np.min(train_features_I)\n",
    "train_features_I = train_features_I + abs(min_valueI)\n",
    "min_valueTI = np.min(test_features_I)\n",
    "test_features_I = test_features_I + abs(min_valueTI)\n",
    "# Naive Bayess\n",
    "NaiveB = MultinomialNB()\n",
    "NaiveB.fit(X_train, y_train)\n",
    "\n",
    "# NaiveB Forest with LSA\n",
    "NaiveBLSA = MultinomialNB()\n",
    "NaiveBLSA.fit(train_features, y_train)\n",
    "\n",
    "# NaiveB Forest Inverse\n",
    "NaiveB_I = MultinomialNB()\n",
    "NaiveB_I.fit(X_train_I, y_train_I)\n",
    "\n",
    "# NaiveB Forest with LSA and INV\n",
    "NaiveBLSA_I = MultinomialNB()\n",
    "NaiveBLSA_I.fit(train_features_I, y_train_I)\n",
    "\n",
    "# Predict labels for test data with LSA\n",
    "predicted_labels_NaiveB_LSA = NaiveBLSA.predict(test_features)\n",
    "\n",
    "# Predict labels for test data\n",
    "predicted_labels_NaiveB = NaiveB.predict(X_test)\n",
    "\n",
    "# Predict labels for test data with LSA and Inverse\n",
    "predicted_labels_NaiveB_LSA_I = NaiveBLSA_I.predict(test_features_I)\n",
    "\n",
    "# Predict labels for test data\n",
    "predicted_labels_NaiveB_I = NaiveB_I.predict(X_test_I)\n",
    "\n",
    "# Evaluate  Naive B classifier with LSA\n",
    "NaiveB_accuracy_LSA = accuracy_score(y_test, predicted_labels_NaiveB_LSA)\n",
    "NaiveB_report_LSA = classification_report(y_test, predicted_labels_NaiveB_LSA)\n",
    "NaiveB_matrix_LSA = confusion_matrix(y_test, predicted_labels_NaiveB_LSA)\n",
    "# Evaluate  Naive B classifier\n",
    "NaiveB_accuracy = accuracy_score(y_test, predicted_labels_NaiveB)\n",
    "NaiveB_report = classification_report(y_test, predicted_labels_NaiveB)\n",
    "NaiveB_matrix = confusion_matrix(y_test, predicted_labels_NaiveB)\n",
    "# Evaluate  NaiveB classifier with Inverse and LSA\n",
    "NaiveB_accuracy_LSA_I = accuracy_score(y_test_I, predicted_labels_NaiveB_LSA_I)\n",
    "NaiveB_report_LSA_I = classification_report(y_test_I, predicted_labels_NaiveB_LSA_I)\n",
    "NaiveB_matrix_LSA_I = confusion_matrix(y_test_I, predicted_labels_NaiveB_LSA_I)\n",
    "# Evaluate  Naive B classifier with Inverse\n",
    "NaiveB_accuracy_I = accuracy_score(y_test_I, predicted_labels_NaiveB_I)\n",
    "NaiveB_report_I = classification_report(y_test_I, predicted_labels_NaiveB_I)\n",
    "NaiveB_matrix_I = confusion_matrix(y_test_I, predicted_labels_NaiveB_I)\n",
    "\n",
    "# Plot conf matrixes\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12,4))\n",
    "ax = ax.flatten()\n",
    "NaiveBMat = sns.heatmap(NaiveB_matrix, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[0])\n",
    "NaiveBMatLSA = sns.heatmap(NaiveB_matrix_LSA, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[1])\n",
    "NaiveBMat_I = sns.heatmap(NaiveB_matrix_I, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[2])\n",
    "NaiveBMatLSA_I = sns.heatmap(NaiveB_matrix_LSA_I, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[3])\n",
    "ax[0].set_title('Naive Bayess TF')\n",
    "ax[0].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[0].set_ylabel('Tikra klasė', fontsize=10)\n",
    "ax[1].set_title('Naive Bayess TF ir LSA')\n",
    "ax[1].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[1].set_ylabel('Tikra klasė', fontsize=10)\n",
    "ax[2].set_title('Naive Bayess TF-IDF')\n",
    "ax[2].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[2].set_ylabel('Tikra klasė', fontsize=10)\n",
    "ax[3].set_title('Naive Bayess TF-IDF ir LSA')\n",
    "ax[3].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[3].set_ylabel('Tikra klasė', fontsize=10)\n",
    "\n",
    "# Adjust spacing between subplots and between subplots and figure edges\n",
    "# Add a title to the figure\n",
    "plt.subplots_adjust(wspace=0.25, hspace=0.8, top=0.92, bottom=0.08, left=0.08, right=0.92)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Print Statistics\n",
    "print(f\"Naive Bayess \\n Accuracy: {NaiveB_accuracy:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",NaiveB_report)\n",
    "print(f\"Naive Bayess LSA \\n Accuracy: {NaiveB_accuracy_LSA:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",NaiveB_report_LSA)\n",
    "print(f\"Naive Bayess Inverse \\n Accuracy: {NaiveB_accuracy_I:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",NaiveB_report_I)\n",
    "print(f\"Naive Bayess Inverse LSA \\n Accuracy: {NaiveB_accuracy_LSA_I:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",NaiveB_report_LSA_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_components_range = range(50, 1001, 50)\n",
    "accuracies = []\n",
    "reports = []\n",
    "matrices = []\n",
    "accuraciesI = []\n",
    "reportsI = []\n",
    "matricesI = []\n",
    "# Initialize lists to store evaluation results\n",
    "for n_components in n_components_range:\n",
    "    lsa = TruncatedSVD(n_components=n_components)\n",
    "    train_features = lsa.fit_transform(X_train)\n",
    "    test_features = lsa.transform(X_test)\n",
    "    lsa = TruncatedSVD(n_components=n_components)\n",
    "    train_features_I = lsa.fit_transform(X_train_I)\n",
    "    test_features_I = lsa.transform(X_test_I)\n",
    "    min_value = np.min(train_features)\n",
    "    train_features = train_features + abs(min_value)\n",
    "    min_valueT = np.min(test_features)\n",
    "    test_features = test_features + abs(min_valueT)\n",
    "    min_valueI = np.min(train_features_I)\n",
    "    train_features_I = train_features_I + abs(min_valueI)\n",
    "    min_valueTI = np.min(test_features_I)\n",
    "    test_features_I = test_features_I + abs(min_valueTI)\n",
    "    NaiveBLSA = MultinomialNB()\n",
    "    NaiveBLSA.fit(train_features, y_train)\n",
    "    NaiveBLSA_Inverse = MultinomialNB()\n",
    "    NaiveBLSA_Inverse.fit(train_features_I, y_train_I)\n",
    "    predicted_labels_NaiveB_LSA = NaiveBLSA.predict(test_features)\n",
    "    predicted_labels_NaiveB_LSA_I = NaiveBLSA_Inverse.predict(test_features_I)\n",
    "    NaiveB_accuracy_LSA = accuracy_score(y_test, predicted_labels_NaiveB_LSA)\n",
    "    NaiveB_report_LSA = classification_report(y_test, predicted_labels_NaiveB_LSA)\n",
    "    NaiveB_matrix_LSA = confusion_matrix(y_test, predicted_labels_NaiveB_LSA)\n",
    "    NaiveB_accuracy_LSA_I = accuracy_score(y_test_I, predicted_labels_NaiveB_LSA_I)\n",
    "    NaiveB_report_LSA_I = classification_report(y_test_I, predicted_labels_NaiveB_LSA_I)\n",
    "    NaiveB_matrix_LSA_I = confusion_matrix(y_test_I, predicted_labels_NaiveB_LSA_I)\n",
    "    accuracies.append(NaiveB_accuracy_LSA)\n",
    "    reports.append(NaiveB_report_LSA)\n",
    "    matrices.append(NaiveB_matrix_LSA)\n",
    "    accuraciesI.append(NaiveB_accuracy_LSA_I)\n",
    "    reportsI.append(NaiveB_report_LSA_I)\n",
    "    matricesI.append(NaiveB_matrix_LSA_I)\n",
    "for accuracy in accuracies:\n",
    "    print(accuracy)\n",
    "for accuracy in accuraciesI:\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Trees\n",
    "DecisionT = DecisionTreeClassifier(min_samples_split=2, max_depth=30)\n",
    "DecisionT.fit(X_train, y_train)\n",
    "\n",
    "# DecisionT with LSA\n",
    "DecisionTLSA = DecisionTreeClassifier(min_samples_split=2, max_depth=30)\n",
    "DecisionTLSA.fit(train_features, y_train)\n",
    "\n",
    "# DecisionT Inverse\n",
    "DecisionT_I = DecisionTreeClassifier(min_samples_split=2, max_depth=30)\n",
    "DecisionT_I.fit(X_train_I, y_train_I)\n",
    "\n",
    "# DecisionT with LSA and INV\n",
    "DecisionTLSA_I = DecisionTreeClassifier(min_samples_split=2, max_depth=30)\n",
    "DecisionTLSA_I.fit(train_features_I, y_train_I)\n",
    "\n",
    "# Predict labels for test data with LSA\n",
    "predicted_labels_DecisionT_LSA = DecisionTLSA.predict(test_features)\n",
    "\n",
    "# Predict labels for test data\n",
    "predicted_labels_DecisionT = DecisionT.predict(X_test)\n",
    "\n",
    "# Predict labels for test data with LSA and Inverse\n",
    "predicted_labels_DecisionT_LSA_I = DecisionTLSA_I.predict(test_features_I)\n",
    "\n",
    "# Predict labels for test data\n",
    "predicted_labels_DecisionT_I = DecisionT_I.predict(X_test_I)\n",
    "\n",
    "# Evaluate  Naive B classifier with LSA\n",
    "DecisionT_accuracy_LSA = accuracy_score(y_test, predicted_labels_DecisionT_LSA)\n",
    "DecisionT_report_LSA = classification_report(y_test, predicted_labels_DecisionT_LSA)\n",
    "DecisionT_matrix_LSA = confusion_matrix(y_test, predicted_labels_DecisionT_LSA)\n",
    "# Evaluate  Naive B classifier\n",
    "DecisionT_accuracy = accuracy_score(y_test, predicted_labels_DecisionT)\n",
    "DecisionT_report = classification_report(y_test, predicted_labels_DecisionT)\n",
    "DecisionT_matrix = confusion_matrix(y_test, predicted_labels_DecisionT)\n",
    "# Evaluate  DecisionT classifier with Inverse and LSA\n",
    "DecisionT_accuracy_LSA_I = accuracy_score(y_test_I, predicted_labels_DecisionT_LSA_I)\n",
    "DecisionT_report_LSA_I = classification_report(y_test_I, predicted_labels_DecisionT_LSA_I)\n",
    "DecisionT_matrix_LSA_I = confusion_matrix(y_test_I, predicted_labels_DecisionT_LSA_I)\n",
    "# Evaluate  Naive B classifier with Inverse\n",
    "DecisionT_accuracy_I = accuracy_score(y_test_I, predicted_labels_DecisionT_I)\n",
    "DecisionT_report_I = classification_report(y_test_I, predicted_labels_DecisionT_I)\n",
    "DecisionT_matrix_I = confusion_matrix(y_test_I, predicted_labels_DecisionT_I)\n",
    "\n",
    "# Plot conf matrixes\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12,4))\n",
    "ax = ax.flatten()\n",
    "DecisionTMat = sns.heatmap(DecisionT_matrix, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[0])\n",
    "DecisionTMatLSA = sns.heatmap(DecisionT_matrix_LSA, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[1])\n",
    "DecisionTMat_I = sns.heatmap(DecisionT_matrix_I, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[2])\n",
    "DecisionTMatLSA_I = sns.heatmap(DecisionT_matrix_LSA_I, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[3])\n",
    "ax[0].set_title('Decision Trees TF')\n",
    "ax[0].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[0].set_ylabel('Tikra klasė', fontsize=10)\n",
    "ax[1].set_title('Decision Trees TF ir LSA')\n",
    "ax[1].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[1].set_ylabel('Tikra klasė', fontsize=10)\n",
    "ax[2].set_title('Decision Trees TF-IDF')\n",
    "ax[2].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[2].set_ylabel('Tikra klasė', fontsize=10)\n",
    "ax[3].set_title('Decision Trees TF-IDF ir LSA')\n",
    "ax[3].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[3].set_ylabel('Tikra klasė', fontsize=10)\n",
    "\n",
    "# Adjust spacing between subplots and between subplots and figure edges\n",
    "# Add a title to the figure\n",
    "plt.subplots_adjust(wspace=0.25, hspace=0.8, top=0.92, bottom=0.08, left=0.08, right=0.92)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Print Statistics\n",
    "print(f\"Decision Trees \\n Accuracy: {DecisionT_accuracy:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",DecisionT_report)\n",
    "print(f\"Decision Trees LSA \\n Accuracy: {DecisionT_accuracy_LSA:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",DecisionT_report_LSA)\n",
    "print(f\"Decision Trees Inverse \\n Accuracy: {DecisionT_accuracy_I:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",DecisionT_report_I)\n",
    "print(f\"Decision Trees Inverse LSA \\n Accuracy: {DecisionT_accuracy_LSA_I:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",DecisionT_report_LSA_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components_range = range(50, 1001, 50)\n",
    "accuracies = []\n",
    "reports = []\n",
    "matrices = []\n",
    "accuraciesI = []\n",
    "reportsI = []\n",
    "matricesI = []\n",
    "# Initialize lists to store evaluation results\n",
    "for n_components in n_components_range:\n",
    "    lsa = TruncatedSVD(n_components=n_components)\n",
    "    train_features = lsa.fit_transform(X_train)\n",
    "    test_features = lsa.transform(X_test)\n",
    "    lsa = TruncatedSVD(n_components=n_components)\n",
    "    train_features_I = lsa.fit_transform(X_train_I)\n",
    "    test_features_I = lsa.transform(X_test_I)\n",
    "    DecisionTLSA = DecisionTreeClassifier(min_samples_split=2, max_depth=30)\n",
    "    DecisionTLSA.fit(train_features, y_train)\n",
    "    DecisionTLSA_Inverse = DecisionTreeClassifier(min_samples_split=2, max_depth=30)\n",
    "    DecisionTLSA_Inverse.fit(train_features_I, y_train_I)\n",
    "    predicted_labels_DecisionT_LSA = DecisionTLSA.predict(test_features)\n",
    "    predicted_labels_DecisionT_LSA_I = DecisionTLSA_Inverse.predict(test_features_I)\n",
    "    DecisionT_accuracy_LSA = accuracy_score(y_test, predicted_labels_DecisionT_LSA)\n",
    "    DecisionT_report_LSA = classification_report(y_test, predicted_labels_DecisionT_LSA)\n",
    "    DecisionT_matrix_LSA = confusion_matrix(y_test, predicted_labels_DecisionT_LSA)\n",
    "    DecisionT_accuracy_LSA_I = accuracy_score(y_test_I, predicted_labels_DecisionT_LSA_I)\n",
    "    DecisionT_report_LSA_I = classification_report(y_test_I, predicted_labels_DecisionT_LSA_I)\n",
    "    DecisionT_matrix_LSA_I = confusion_matrix(y_test_I, predicted_labels_DecisionT_LSA_I)\n",
    "    accuracies.append(DecisionT_accuracy_LSA)\n",
    "    reports.append(DecisionT_report_LSA)\n",
    "    matrices.append(DecisionT_matrix_LSA)\n",
    "    accuraciesI.append(DecisionT_accuracy_LSA_I)\n",
    "    reportsI.append(DecisionT_report_LSA_I)\n",
    "    matricesI.append(DecisionT_matrix_LSA_I)\n",
    "for accuracy in accuracies:\n",
    "    print(accuracy)\n",
    "for accuracy in accuraciesI:\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron\n",
    "MPLPercep = MLPClassifier(hidden_layer_sizes=(350,500))\n",
    "MPLPercep.fit(X_train, y_train)\n",
    "\n",
    "# MPLPercep with LSA\n",
    "MPLPercepLSA = MLPClassifier(hidden_layer_sizes=(350,500))\n",
    "MPLPercepLSA.fit(train_features, y_train)\n",
    "\n",
    "# MPLPercep Inverse\n",
    "MPLPercep_I = MLPClassifier(hidden_layer_sizes=(350,500))\n",
    "MPLPercep_I.fit(X_train_I, y_train_I)\n",
    "\n",
    "# MPLPercep with LSA and INV\n",
    "MPLPercepLSA_I = MLPClassifier(hidden_layer_sizes=(350,500))\n",
    "MPLPercepLSA_I.fit(train_features_I, y_train_I)\n",
    "\n",
    "# Predict labels for test data with LSA\n",
    "predicted_labels_MPLPercep_LSA = MPLPercepLSA.predict(test_features)\n",
    "\n",
    "# Predict labels for test data\n",
    "predicted_labels_MPLPercep = MPLPercep.predict(X_test)\n",
    "\n",
    "# Predict labels for test data with LSA and Inverse\n",
    "predicted_labels_MPLPercep_LSA_I = MPLPercepLSA_I.predict(test_features_I)\n",
    "\n",
    "# Predict labels for test data\n",
    "predicted_labels_MPLPercep_I = MPLPercep_I.predict(X_test_I)\n",
    "\n",
    "# Evaluate  Naive B classifier with LSA\n",
    "MPLPercep_accuracy_LSA = accuracy_score(y_test, predicted_labels_MPLPercep_LSA)\n",
    "MPLPercep_report_LSA = classification_report(y_test, predicted_labels_MPLPercep_LSA)\n",
    "MPLPercep_matrix_LSA = confusion_matrix(y_test, predicted_labels_MPLPercep_LSA)\n",
    "# Evaluate  Naive B classifier\n",
    "MPLPercep_accuracy = accuracy_score(y_test, predicted_labels_MPLPercep)\n",
    "MPLPercep_report = classification_report(y_test, predicted_labels_MPLPercep)\n",
    "MPLPercep_matrix = confusion_matrix(y_test, predicted_labels_MPLPercep)\n",
    "# Evaluate  MPLPercep classifier with Inverse and LSA\n",
    "MPLPercep_accuracy_LSA_I = accuracy_score(y_test_I, predicted_labels_MPLPercep_LSA_I)\n",
    "MPLPercep_report_LSA_I = classification_report(y_test_I, predicted_labels_MPLPercep_LSA_I)\n",
    "MPLPercep_matrix_LSA_I = confusion_matrix(y_test_I, predicted_labels_MPLPercep_LSA_I)\n",
    "# Evaluate  Naive B classifier with Inverse\n",
    "MPLPercep_accuracy_I = accuracy_score(y_test_I, predicted_labels_MPLPercep_I)\n",
    "MPLPercep_report_I = classification_report(y_test_I, predicted_labels_MPLPercep_I)\n",
    "MPLPercep_matrix_I = confusion_matrix(y_test_I, predicted_labels_MPLPercep_I)\n",
    "\n",
    "# Plot conf matrixes\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12,4))\n",
    "ax = ax.flatten()\n",
    "MPLPercepMat = sns.heatmap(MPLPercep_matrix, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[0])\n",
    "MPLPercepMatLSA = sns.heatmap(MPLPercep_matrix_LSA, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[1])\n",
    "MPLPercepMat_I = sns.heatmap(MPLPercep_matrix_I, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[2])\n",
    "MPLPercepMatLSA_I = sns.heatmap(MPLPercep_matrix_LSA_I, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[3])\n",
    "ax[0].set_title('MLP TF')\n",
    "ax[0].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[0].set_ylabel('Tikra klasė', fontsize=10)\n",
    "ax[1].set_title('MLP TF ir LSA')\n",
    "ax[1].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[1].set_ylabel('Tikra klasė', fontsize=10)\n",
    "ax[2].set_title('MLP TF-IDF')\n",
    "ax[2].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[2].set_ylabel('Tikra klasė', fontsize=10)\n",
    "ax[3].set_title('MLP TF-IDF ir LSA')\n",
    "ax[3].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[3].set_ylabel('Tikra klasė', fontsize=10)\n",
    "\n",
    "# Adjust spacing between subplots and between subplots and figure edges\n",
    "# Add a title to the figure\n",
    "plt.subplots_adjust(wspace=0.25, hspace=0.8, top=0.92, bottom=0.08, left=0.08, right=0.92)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Print Statistics\n",
    "print(f\"MLP Perceptron \\n Accuracy: {MPLPercep_accuracy:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",MPLPercep_report)\n",
    "print(f\"MLP Perceptron LSA \\n Accuracy: {MPLPercep_accuracy_LSA:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",MPLPercep_report_LSA)\n",
    "print(f\"MLP Perceptron Inverse \\n Accuracy: {MPLPercep_accuracy_I:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",MPLPercep_report_I)\n",
    "print(f\"MLP Perceptron Inverse LSA \\n Accuracy: {MPLPercep_accuracy_LSA_I:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",MPLPercep_report_LSA_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components_range = range(50, 1001, 50)\n",
    "accuracies = []\n",
    "reports = []\n",
    "matrices = []\n",
    "accuraciesI = []\n",
    "reportsI = []\n",
    "matricesI = []\n",
    "# Initialize lists to store evaluation results\n",
    "for n_components in n_components_range:\n",
    "    lsa = TruncatedSVD(n_components=n_components)\n",
    "    train_features = lsa.fit_transform(X_train)\n",
    "    test_features = lsa.transform(X_test)\n",
    "    lsa = TruncatedSVD(n_components=n_components)\n",
    "    train_features_I = lsa.fit_transform(X_train_I)\n",
    "    test_features_I = lsa.transform(X_test_I)\n",
    "    PerceptLSA = MLPClassifier(hidden_layer_sizes=(350,500))\n",
    "    PerceptLSA.fit(train_features, y_train)\n",
    "    PerceptLSA_Inverse = MLPClassifier(hidden_layer_sizes=(350,500))\n",
    "    PerceptLSA_Inverse.fit(train_features_I, y_train_I)\n",
    "    predicted_labels_Percept_LSA = PerceptLSA.predict(test_features)\n",
    "    predicted_labels_Percept_LSA_I = PerceptLSA_Inverse.predict(test_features_I)\n",
    "    Percept_accuracy_LSA = accuracy_score(y_test, predicted_labels_Percept_LSA)\n",
    "    Percept_accuracy_LSA_I = accuracy_score(y_test_I, predicted_labels_Percept_LSA_I)\n",
    "    accuracies.append(Percept_accuracy_LSA)\n",
    "    accuraciesI.append(Percept_accuracy_LSA_I)\n",
    "    print(f\"n_components = {n_components}, accuracy = {Percept_accuracy_LSA}, accuracyI = {Percept_accuracy_LSA_I}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for accuracy in accuracies:\n",
    "    print(accuracy)\n",
    "for accuracy in accuraciesI:\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "GradBoost = GradientBoostingClassifier(max_depth=10, n_estimators=100)\n",
    "GradBoost.fit(X_train, y_train)\n",
    "\n",
    "# GradBoost with LSA\n",
    "GradBoostLSA = GradientBoostingClassifier(max_depth=10, n_estimators=100)\n",
    "GradBoostLSA.fit(train_features, y_train)\n",
    "\n",
    "# GradBoost Inverse\n",
    "GradBoost_I = GradientBoostingClassifier(max_depth=10, n_estimators=100)\n",
    "GradBoost_I.fit(X_train_I, y_train_I)\n",
    "\n",
    "# GradBoost with LSA and INV\n",
    "GradBoostLSA_I = GradientBoostingClassifier(max_depth=10, n_estimators=100)\n",
    "GradBoostLSA_I.fit(train_features_I, y_train_I)\n",
    "\n",
    "# Predict labels for test data with LSA\n",
    "predicted_labels_GradBoost_LSA = GradBoostLSA.predict(test_features)\n",
    "\n",
    "# Predict labels for test data\n",
    "predicted_labels_GradBoost = GradBoost.predict(X_test)\n",
    "\n",
    "# Predict labels for test data with LSA and Inverse\n",
    "predicted_labels_GradBoost_LSA_I = GradBoostLSA_I.predict(test_features_I)\n",
    "\n",
    "# Predict labels for test data\n",
    "predicted_labels_GradBoost_I = GradBoost_I.predict(X_test_I)\n",
    "\n",
    "# Evaluate  Naive B classifier with LSA\n",
    "GradBoost_accuracy_LSA = accuracy_score(y_test, predicted_labels_GradBoost_LSA)\n",
    "GradBoost_report_LSA = classification_report(y_test, predicted_labels_GradBoost_LSA)\n",
    "GradBoost_matrix_LSA = confusion_matrix(y_test, predicted_labels_GradBoost_LSA)\n",
    "# Evaluate  Naive B classifier\n",
    "GradBoost_accuracy = accuracy_score(y_test, predicted_labels_GradBoost)\n",
    "GradBoost_report = classification_report(y_test, predicted_labels_GradBoost)\n",
    "GradBoost_matrix = confusion_matrix(y_test, predicted_labels_GradBoost)\n",
    "# Evaluate  GradBoost classifier with Inverse and LSA\n",
    "GradBoost_accuracy_LSA_I = accuracy_score(y_test_I, predicted_labels_GradBoost_LSA_I)\n",
    "GradBoost_report_LSA_I = classification_report(y_test_I, predicted_labels_GradBoost_LSA_I)\n",
    "GradBoost_matrix_LSA_I = confusion_matrix(y_test_I, predicted_labels_GradBoost_LSA_I)\n",
    "# Evaluate  Naive B classifier with Inverse\n",
    "GradBoost_accuracy_I = accuracy_score(y_test_I, predicted_labels_GradBoost_I)\n",
    "GradBoost_report_I = classification_report(y_test_I, predicted_labels_GradBoost_I)\n",
    "GradBoost_matrix_I = confusion_matrix(y_test_I, predicted_labels_GradBoost_I)\n",
    "\n",
    "# Plot conf matrixes\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12,4))\n",
    "ax = ax.flatten()\n",
    "GradBoostMat = sns.heatmap(GradBoost_matrix, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[0])\n",
    "GradBoostMatLSA = sns.heatmap(GradBoost_matrix_LSA, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[1])\n",
    "GradBoostMat_I = sns.heatmap(GradBoost_matrix_I, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[2])\n",
    "GradBoostMatLSA_I = sns.heatmap(GradBoost_matrix_LSA_I, annot=True, cmap='Blues',fmt='d',linewidth=0.01,linecolor=\"#222\", ax=ax[3])\n",
    "ax[0].set_title('Gradient Boosted Trees TF')\n",
    "ax[0].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[0].set_ylabel('Tikra klasė', fontsize=10)\n",
    "ax[1].set_title('Gradient Boosted Trees TF ir LSA')\n",
    "ax[1].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[1].set_ylabel('Tikra klasė', fontsize=10)\n",
    "ax[2].set_title('Gradient Boosted Trees TF-IDF')\n",
    "ax[2].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[2].set_ylabel('Tikra klasė', fontsize=10)\n",
    "ax[3].set_title('Gradient Boosted Trees TF-IDF ir LSA')\n",
    "ax[3].set_xlabel('Spėjimas', fontsize=10)\n",
    "ax[3].set_ylabel('Tikra klasė', fontsize=10)\n",
    "\n",
    "# Adjust spacing between subplots and between subplots and figure edges\n",
    "# Add a title to the figure\n",
    "plt.subplots_adjust(wspace=0.25, hspace=0.8, top=0.92, bottom=0.08, left=0.08, right=0.92)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Print Statistics\n",
    "print(f\"Gradient Boosting Trees \\n Accuracy: {GradBoost_accuracy:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",GradBoost_report)\n",
    "print(f\"Gradient Boosting Trees LSA \\n Accuracy: {GradBoost_accuracy_LSA:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",GradBoost_report_LSA)\n",
    "print(f\"Gradient Boosting Trees Inverse \\n Accuracy: {GradBoost_accuracy_I:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",GradBoost_report_I)\n",
    "print(f\"Gradient Boosting Trees Inverse LSA \\n Accuracy: {GradBoost_accuracy_LSA_I:.2f}\", \"\\n\" ,\"Report: \",  \"\\n\",GradBoost_report_LSA_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components_range = range(50, 1001, 50)\n",
    "accuracies = []\n",
    "reports = []\n",
    "matrices = []\n",
    "accuraciesI = []\n",
    "reportsI = []\n",
    "matricesI = []\n",
    "# Initialize lists to store evaluation results\n",
    "for n_components in n_components_range:\n",
    "    lsa = TruncatedSVD(n_components=n_components)\n",
    "    train_features = lsa.fit_transform(X_train)\n",
    "    test_features = lsa.transform(X_test)\n",
    "    lsa = TruncatedSVD(n_components=n_components)\n",
    "    train_features_I = lsa.fit_transform(X_train_I)\n",
    "    test_features_I = lsa.transform(X_test_I)\n",
    "    GradBoostLSA = GradientBoostingClassifier(max_depth=10, n_estimators=100)\n",
    "    GradBoostLSA.fit(train_features, y_train)\n",
    "    GradBoostLSA_Inverse = GradientBoostingClassifier(max_depth=10, n_estimators=100)\n",
    "    GradBoostLSA_Inverse.fit(train_features_I, y_train_I)\n",
    "    predicted_labels_GradBoost_LSA = GradBoostLSA.predict(test_features)\n",
    "    predicted_labels_GradBoost_LSA_I = GradBoostLSA_Inverse.predict(test_features_I)\n",
    "    GradBoost_accuracy_LSA = accuracy_score(y_test, predicted_labels_GradBoost_LSA)\n",
    "    GradBoost_report_LSA = classification_report(y_test, predicted_labels_GradBoost_LSA)\n",
    "    GradBoost_matrix_LSA = confusion_matrix(y_test, predicted_labels_GradBoost_LSA)\n",
    "    GradBoost_accuracy_LSA_I = accuracy_score(y_test_I, predicted_labels_GradBoost_LSA_I)\n",
    "    GradBoost_report_LSA_I = classification_report(y_test_I, predicted_labels_GradBoost_LSA_I)\n",
    "    GradBoost_matrix_LSA_I = confusion_matrix(y_test_I, predicted_labels_GradBoost_LSA_I)\n",
    "    accuracies.append(GradBoost_accuracy_LSA)\n",
    "    reports.append(GradBoost_report_LSA)\n",
    "    matrices.append(GradBoost_matrix_LSA)\n",
    "    accuraciesI.append(GradBoost_accuracy_LSA_I)\n",
    "    reportsI.append(GradBoost_report_LSA_I)\n",
    "    matricesI.append(GradBoost_matrix_LSA_I)\n",
    "    print(f\"n_components = {n_components}, accuracy = {GradBoost_accuracy_LSA}, accuracyI = {GradBoost_accuracy_LSA_I}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for accuracy in accuracies:\n",
    "    print(accuracy)\n",
    "for accuracy in accuraciesI:\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametrai\n",
    "leafs = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "accuracies = []\n",
    "accuraciesI = []\n",
    "for leaf in leafs: \n",
    "    svm = SVC(C=leaf, kernel='linear', degree=3, gamma='auto')\n",
    "    svm.fit(X_train, y_train)\n",
    "    svm_I = SVC(C=leaf, kernel='linear', degree=3, gamma='auto')\n",
    "    svm_I.fit(X_train_I, y_train_I)\n",
    "    predicted_labels_svm = svm.predict(X_test)\n",
    "    predicted_labels_svm_I = svm_I.predict(X_test_I)\n",
    "    svm_accuracy = accuracy_score(y_test, predicted_labels_svm)\n",
    "    svm_accuracy_I = accuracy_score(y_test_I, predicted_labels_svm_I)\n",
    "    accuracies.append(svm_accuracy)\n",
    "    accuraciesI.append(svm_accuracy_I)\n",
    "\n",
    "for accuracy in accuracies:\n",
    "    print(round(accuracy, 2))\n",
    "print('Inverse \\n')\n",
    "for accuracy in accuraciesI:\n",
    "    print(round(accuracy, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Inverse\n",
    "random_I = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "random_I.fit(X_train_I, y_train_I)\n",
    "randomF = 'randomF.pkl'\n",
    "pickle.dump(random_I, open(randomF, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradBoost with LSA and INV\n",
    "# Inverse\n",
    "\n",
    "GradBoostLSA_I = GradientBoostingClassifier(max_depth=10, n_estimators=100)\n",
    "GradBoostLSA_I.fit(train_features_I, y_train_I)\n",
    "GradBoost = 'GradBoost.pkl'\n",
    "pickle.dump(GradBoostLSA_I, open(GradBoost, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionT_I = DecisionTreeClassifier(min_samples_split=2, max_depth=30)\n",
    "DecisionT_I.fit(X_train_I, y_train_I)\n",
    "decisonT = 'decisionT.pkl'\n",
    "pickle.dump(DecisionT_I, open(decisonT, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM withoutLSA with INV\n",
    "svm_Inverse = SVC(C=0.9, kernel='linear', degree=3, gamma='auto')\n",
    "svm_Inverse.fit(X_train_I, y_train_I)\n",
    "svmmodel = 'svm.pkl'\n",
    "pickle.dump(svm_Inverse, open(svmmodel, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaiveB = MultinomialNB()\n",
    "NaiveB.fit(X_train, y_train)\n",
    "naivebmodel = 'naivebmodel.pkl'\n",
    "pickle.dump(NaiveB, open(naivebmodel, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPLPercep Inverse\n",
    "MPLPercep_I = MLPClassifier(hidden_layer_sizes=(350,500))\n",
    "MPLPercep_I.fit(X_train_I, y_train_I)\n",
    "mlpmodel = 'mlpmodel.pkl'\n",
    "pickle.dump(MPLPercep_I, open(mlpmodel, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = pickle.load(open('vectorizer.pkl', 'rb'))\n",
    "vectorizerI = pickle.load(open('vetorizerI.pkl', 'rb'))\n",
    "lsa = pickle.load(open('lsa.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pickle.load(open('decisionT.pkl', 'rb'))\n",
    "nb = pickle.load(open('naivebmodel.pkl', 'rb'))\n",
    "svm = pickle.load(open('svm.pkl', 'rb'))\n",
    "rt = pickle.load(open('randomF.pkl', 'rb'))\n",
    "mlp = pickle.load(open('mlpmodel.pkl', 'rb'))\n",
    "gb = pickle.load(open('GradBoost.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "various_sentences = [\n",
    "    \"Zala patenkintas visada yra,vanagas jau skundziasi tai jau sloguoja neisimegojes visa laika jam problemos aplinkui\",\n",
    "    \"viskas gerai nepanikuokit stebetojai ,šaunuolis visad su jumoru nenusimines\",\n",
    "    \"Svarbiausia, kad nuotaikos ir pozityvo nenusimušo\",\n",
    "    \"reikėtų juos apdaužyti loxus\",\n",
    "    \"suspardysiu tave gaidį\",\n",
    "    \"jeigu tave surasčiau, tai sudaužyčiau\",\n",
    "    \"kategorijos, kurių skonis keičiasi priklausomai nuo to, kokį pagrindinį priedą pasirinksi\",\n",
    "    \"dar parėkausi ir baigsis blogai tau\",\n",
    "    \"atvažiuosiu pas tave ir primušiu\",\n",
    "    \"15min prenumerata: https://15min.lt/prenumerata/apie\",\n",
    "    \"loxas tu eisi, butum is mano miesto sudauzyciau tave\",\n",
    "    \"puikus papildas turintis įvairių formų baltymų, specialiai pritaikytas aktyvioms moterims.\",\n",
    "    \"uzmusti reikia tave\",\n",
    "    \"Geriausi PASKUTINĖS MINUTĖS pasiūlymai Graikijoje\"\n",
    "]\n",
    "various_sentences_labels = [0,0,0,1,1,1,0,1,1,0,1,0,1,0]\n",
    "for i in range(len(various_sentences)):\n",
    "    correct_sentence = sentence_corection(various_sentences[i])\n",
    "    predictionOnSVM = svm.predict(vectorizerI.transform([correct_sentence]))\n",
    "    predictionOnRandom = rt.predict(vectorizerI.transform([correct_sentence]))\n",
    "    predictionOnDT = dt.predict(vectorizerI.transform([correct_sentence]))\n",
    "    predictionOnMLP = mlp.predict(vectorizerI.transform([correct_sentence]))\n",
    "    predictionOnNB = nb.predict(vectorizer.transform([correct_sentence]))\n",
    "    predictionOnGT = gb.predict(lsa.transform(vectorizerI.transform([correct_sentence])))\n",
    "    print(f\"{i+1} SVM :  {predictionOnSVM[0]} | RF:  {predictionOnRandom[0]} | NB:  {predictionOnNB[0]} DT: {predictionOnDT[0]} | GB: {predictionOnGT[0]} | MLP: {predictionOnMLP[0]}  Real Class: {various_sentences_labels[i]}\\n\")\n",
    "for i in range(len(various_sentences)):\n",
    "    print(f\"{i+1}. {various_sentences[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
